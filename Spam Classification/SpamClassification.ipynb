{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "I0nTcE2qp0Y2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\n",
        "    \"SMSSpamCollection\",\n",
        "    sep=\"\\t\",\n",
        "    header=None,\n",
        "    names=[\"label\", \"message\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display basic information about the dataset\n",
        "print(\"-------------------- HEAD --------------------\")\n",
        "print(df.head())\n",
        "print(\"-------------------- DESCRIBE --------------------\")\n",
        "print(df.describe())\n",
        "print(\"-------------------- INFO --------------------\")\n",
        "print(df.info())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zhbpmr53qiXD",
        "outputId": "64e0ddc7-34b7-4d02-b525-c895d8ae7ab7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------- HEAD --------------------\n",
            "  label                                            message\n",
            "0   ham  Go until jurong point, crazy.. Available only ...\n",
            "1   ham                      Ok lar... Joking wif u oni...\n",
            "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
            "3   ham  U dun say so early hor... U c already then say...\n",
            "4   ham  Nah I don't think he goes to usf, he lives aro...\n",
            "-------------------- DESCRIBE --------------------\n",
            "       label                 message\n",
            "count   5572                    5572\n",
            "unique     2                    5169\n",
            "top      ham  Sorry, I'll call later\n",
            "freq    4825                      30\n",
            "-------------------- INFO --------------------\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 5572 entries, 0 to 5571\n",
            "Data columns (total 2 columns):\n",
            " #   Column   Non-Null Count  Dtype \n",
            "---  ------   --------------  ----- \n",
            " 0   label    5572 non-null   object\n",
            " 1   message  5572 non-null   object\n",
            "dtypes: object(2)\n",
            "memory usage: 87.2+ KB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values\n",
        "print(\"Missing values:\\n\", df.isnull().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0FQNO01q5BD",
        "outputId": "d249728d-3945-4fd8-f9b5-170edad2015b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing values:\n",
            " label      0\n",
            "message    0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for duplicates\n",
        "print(\"Duplicate entries:\", df.duplicated().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGryRg_oq_kF",
        "outputId": "4ee859f3-9703-42cf-d936-77940021945e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Duplicate entries: 403\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove duplicates if any\n",
        "df = df.drop_duplicates()"
      ],
      "metadata": {
        "id": "SRoZDGulrBpP"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# Download the necessary NLTK data files\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"punkt_tab\")\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "print(\"=== BEFORE ANY PREPROCESSING ===\")\n",
        "print(df.head(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QlCycuOArUu2",
        "outputId": "76508609-f7f4-4676-9566-711c635c6d85"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== BEFORE ANY PREPROCESSING ===\n",
            "  label                                            message\n",
            "0   ham  Go until jurong point, crazy.. Available only ...\n",
            "1   ham                      Ok lar... Joking wif u oni...\n",
            "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
            "3   ham  U dun say so early hor... U c already then say...\n",
            "4   ham  Nah I don't think he goes to usf, he lives aro...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert all message text to lowercase\n",
        "df[\"message\"] = df[\"message\"].str.lower()\n",
        "print(\"\\n=== AFTER LOWERCASING ===\")\n",
        "print(df[\"message\"].head(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7tVN-6W6sQ5U",
        "outputId": "685ab751-8841-4f92-f72e-6d84c614b627"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== AFTER LOWERCASING ===\n",
            "0    go until jurong point, crazy.. available only ...\n",
            "1                        ok lar... joking wif u oni...\n",
            "2    free entry in 2 a wkly comp to win fa cup fina...\n",
            "3    u dun say so early hor... u c already then say...\n",
            "4    nah i don't think he goes to usf, he lives aro...\n",
            "Name: message, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Remove non-essential punctuation and numbers, keep useful symbols like $ and !\n",
        "df[\"message\"] = df[\"message\"].apply(lambda x: re.sub(r\"[^a-z\\s$!]\", \"\", x))\n",
        "print(\"\\n=== AFTER REMOVING PUNCTUATION & NUMBERS (except $ and !) ===\")\n",
        "print(df[\"message\"].head(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PVoEcxAYse3l",
        "outputId": "8e42b505-6112-4688-ff33-3eb129b6a7e2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== AFTER REMOVING PUNCTUATION & NUMBERS (except $ and !) ===\n",
            "0    go until jurong point crazy available only in ...\n",
            "1                              ok lar joking wif u oni\n",
            "2    free entry in  a wkly comp to win fa cup final...\n",
            "3          u dun say so early hor u c already then say\n",
            "4    nah i dont think he goes to usf he lives aroun...\n",
            "Name: message, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Split each message into individual tokens\n",
        "df[\"message\"] = df[\"message\"].apply(word_tokenize)\n",
        "print(\"\\n=== AFTER TOKENIZATION ===\")\n",
        "print(df[\"message\"].head(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ekpDzinns7tN",
        "outputId": "ade56a99-16e0-4bf2-ca9e-6a8256361822"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== AFTER TOKENIZATION ===\n",
            "0    [go, until, jurong, point, crazy, available, o...\n",
            "1                       [ok, lar, joking, wif, u, oni]\n",
            "2    [free, entry, in, a, wkly, comp, to, win, fa, ...\n",
            "3    [u, dun, say, so, early, hor, u, c, already, t...\n",
            "4    [nah, i, dont, think, he, goes, to, usf, he, l...\n",
            "Name: message, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Define a set of English stop words and remove them from the tokens\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "df[\"message\"] = df[\"message\"].apply(lambda x: [word for word in x if word not in stop_words])\n",
        "print(\"\\n=== AFTER REMOVING STOP WORDS ===\")\n",
        "print(df[\"message\"].head(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EepOF1lxs9J-",
        "outputId": "e24c9785-0ec2-41a5-ef23-f071a325556f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== AFTER REMOVING STOP WORDS ===\n",
            "0    [go, jurong, point, crazy, available, bugis, n...\n",
            "1                       [ok, lar, joking, wif, u, oni]\n",
            "2    [free, entry, wkly, comp, win, fa, cup, final,...\n",
            "3        [u, dun, say, early, hor, u, c, already, say]\n",
            "4    [nah, dont, think, goes, usf, lives, around, t...\n",
            "Name: message, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Stem each token to reduce words to their base form\n",
        "stemmer = PorterStemmer()\n",
        "df[\"message\"] = df[\"message\"].apply(lambda x: [stemmer.stem(word) for word in x])\n",
        "print(\"\\n=== AFTER STEMMING ===\")\n",
        "print(df[\"message\"].head(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8VkdTBOtH5s",
        "outputId": "4530497b-8540-4996-cbe2-569a21c9618f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== AFTER STEMMING ===\n",
            "0    [go, jurong, point, crazi, avail, bugi, n, gre...\n",
            "1                         [ok, lar, joke, wif, u, oni]\n",
            "2    [free, entri, wkli, comp, win, fa, cup, final,...\n",
            "3        [u, dun, say, earli, hor, u, c, alreadi, say]\n",
            "4    [nah, dont, think, goe, usf, live, around, tho...\n",
            "Name: message, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Rejoin tokens into a single string for feature extraction\n",
        "df[\"message\"] = df[\"message\"].apply(lambda x: \" \".join(x))\n",
        "print(\"\\n=== AFTER JOINING TOKENS BACK INTO STRINGS ===\")\n",
        "print(df[\"message\"].head(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mnzc8aw9tOiO",
        "outputId": "004e6976-32b1-4d38-d3f9-d38600df7eb6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== AFTER JOINING TOKENS BACK INTO STRINGS ===\n",
            "0    go jurong point crazi avail bugi n great world...\n",
            "1                                ok lar joke wif u oni\n",
            "2    free entri wkli comp win fa cup final tkt st m...\n",
            "3                  u dun say earli hor u c alreadi say\n",
            "4            nah dont think goe usf live around though\n",
            "Name: message, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A common way to represent text numerically is through a bag-of-words model. This technique constructs a vocabulary of unique terms from the dataset and represents each message as a vector of term counts. Each element in the vector corresponds to a term in the vocabulary, and its value indicates how often that term appears in the message.\n",
        "\n",
        "Using only unigrams (individual words) does not preserve the original word order; it treats each document as a collection of terms and their frequencies, independent of sequence.\n",
        "\n",
        "To introduce a limited sense of order, we also include bigrams, which are pairs of consecutive words. By incorporating bigrams, we capture some local ordering information.\n",
        "\n",
        "For example, the bigram free prize might help distinguish a spam message promising a reward from a simple statement containing the word free alone. However, beyond these small sequences, the global order of words and sentence structure remains largely lost. In other words, CountVectorizer does not preserve complete word order; it only captures localized patterns defined by the chosen ngram_range.\n",
        "\n",
        "CountVectorizer from the scikit-learn library efficiently implements the bag-of-words approach. It converts a collection of documents into a matrix of term counts, where each row represents a message and each column corresponds to a term (unigram or bigram). Before transformation, CountVectorizer applies tokenization, builds a vocabulary, and then maps each document to a numeric vector.\n",
        "\n",
        "Key parameters for refining the feature set:\n",
        "\n",
        "- `min_df=1`: A term must appear in at least one document to be included. While this threshold is set to 1 here, higher values can be used in practice to exclude rare terms.\n",
        "- `max_df=0.9`: Terms that appear in more than 90% of the documents are excluded, removing overly common words that provide limited differentiation.\n",
        "- `ngram_range=(1, 2)`: The feature matrix captures individual words and common word pairs by including unigrams and bigrams, potentially improving the modelâ€™s ability to detect spam patterns.\n"
      ],
      "metadata": {
        "id": "LBdOQry-udDV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Initialize CountVectorizer with bigrams, min_df, and max_df to focus on relevant terms\n",
        "vectorizer = CountVectorizer(min_df=1, max_df=0.9, ngram_range=(1, 2))\n",
        "\n",
        "# Fit and transform the message column\n",
        "X = vectorizer.fit_transform(df[\"message\"])\n",
        "\n",
        "# Labels (target variable)\n",
        "y = df[\"label\"].apply(lambda x: 1 if x == \"spam\" else 0)  # Converting labels to 1 and 0"
      ],
      "metadata": {
        "id": "ZlvXDqdjuXYS"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Build the pipeline by combining vectorization and classification\n",
        "pipeline = Pipeline([\n",
        "    (\"vectorizer\", vectorizer),\n",
        "    (\"classifier\", MultinomialNB())\n",
        "])"
      ],
      "metadata": {
        "id": "EGekDjVqvOjn"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the parameter grid for hyperparameter tuning\n",
        "param_grid = {\n",
        "    \"classifier__alpha\": [0.01, 0.1, 0.15, 0.2, 0.25, 0.5, 0.75, 1.0]\n",
        "}\n",
        "\n",
        "# Perform the grid search with 5-fold cross-validation and the F1-score as metric\n",
        "grid_search = GridSearchCV(\n",
        "    pipeline,\n",
        "    param_grid,\n",
        "    cv=5,\n",
        "    scoring=\"f1\"\n",
        ")\n",
        "\n",
        "# Fit the grid search on the full dataset\n",
        "grid_search.fit(df[\"message\"], y)\n",
        "\n",
        "# Extract the best model identified by the grid search\n",
        "best_model = grid_search.best_estimator_\n",
        "print(\"Best model parameters:\", grid_search.best_params_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wENMcRvevQB8",
        "outputId": "1660be59-8a26-4283-d502-679280b21bb6"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model parameters: {'classifier__alpha': 0.25}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example SMS messages for evaluation\n",
        "new_messages = [\n",
        "    \"Congratulations! You've won a $1000 Walmart gift card. Go to http://bit.ly/1234 to claim now.\",\n",
        "    \"Hey, are we still meeting up for lunch today?\",\n",
        "    \"Urgent! Your account has been compromised. Verify your details here: www.fakebank.com/verify\",\n",
        "    \"Reminder: Your appointment is scheduled for tomorrow at 10am.\",\n",
        "    \"FREE entry in a weekly competition to win an iPad. Just text WIN to 80085 now!\",\n",
        "]"
      ],
      "metadata": {
        "id": "-_Kv7qRyvnzu"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import re\n",
        "\n",
        "# Preprocess function that mirrors the training-time preprocessing\n",
        "def preprocess_message(message):\n",
        "    message = message.lower()\n",
        "    message = re.sub(r\"[^a-z\\s$!]\", \"\", message)\n",
        "    tokens = word_tokenize(message)\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    tokens = [stemmer.stem(word) for word in tokens]\n",
        "    return \" \".join(tokens)"
      ],
      "metadata": {
        "id": "NnHZJ282vsdJ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess and vectorize messages\n",
        "processed_messages = [preprocess_message(msg) for msg in new_messages]"
      ],
      "metadata": {
        "id": "t-apCAn4vxmH"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform preprocessed messages into feature vectors\n",
        "X_new = best_model.named_steps[\"vectorizer\"].transform(processed_messages)"
      ],
      "metadata": {
        "id": "_ZbbH2wEv3_1"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict with the trained classifier\n",
        "predictions = best_model.named_steps[\"classifier\"].predict(X_new)\n",
        "prediction_probabilities = best_model.named_steps[\"classifier\"].predict_proba(X_new)"
      ],
      "metadata": {
        "id": "artoJwgLv_Ij"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display predictions and probabilities for each evaluated message\n",
        "for i, msg in enumerate(new_messages):\n",
        "    prediction = \"Spam\" if predictions[i] == 1 else \"Not-Spam\"\n",
        "    spam_probability = prediction_probabilities[i][1]  # Probability of being spam\n",
        "    ham_probability = prediction_probabilities[i][0]   # Probability of being not spam\n",
        "\n",
        "    print(f\"Message: {msg}\")\n",
        "    print(f\"Prediction: {prediction}\")\n",
        "    print(f\"Spam Probability: {spam_probability:.2f}\")\n",
        "    print(f\"Not-Spam Probability: {ham_probability:.2f}\")\n",
        "    print(\"-\" * 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_gcFEPotwFCE",
        "outputId": "091f1d68-c41c-4d17-906c-bdc8f9e1e074"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Message: Congratulations! You've won a $1000 Walmart gift card. Go to http://bit.ly/1234 to claim now.\n",
            "Prediction: Spam\n",
            "Spam Probability: 1.00\n",
            "Not-Spam Probability: 0.00\n",
            "--------------------------------------------------\n",
            "Message: Hey, are we still meeting up for lunch today?\n",
            "Prediction: Not-Spam\n",
            "Spam Probability: 0.00\n",
            "Not-Spam Probability: 1.00\n",
            "--------------------------------------------------\n",
            "Message: Urgent! Your account has been compromised. Verify your details here: www.fakebank.com/verify\n",
            "Prediction: Spam\n",
            "Spam Probability: 0.96\n",
            "Not-Spam Probability: 0.04\n",
            "--------------------------------------------------\n",
            "Message: Reminder: Your appointment is scheduled for tomorrow at 10am.\n",
            "Prediction: Not-Spam\n",
            "Spam Probability: 0.00\n",
            "Not-Spam Probability: 1.00\n",
            "--------------------------------------------------\n",
            "Message: FREE entry in a weekly competition to win an iPad. Just text WIN to 80085 now!\n",
            "Prediction: Spam\n",
            "Spam Probability: 1.00\n",
            "Not-Spam Probability: 0.00\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Save the trained model to a file for future use\n",
        "model_filename = 'spam_detection_model.joblib'\n",
        "joblib.dump(best_model, model_filename)\n",
        "\n",
        "print(f\"Model saved to {model_filename}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_tLGNMSwwS_0",
        "outputId": "51effd12-bf04-403f-d369-a0744dbb2aff"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to spam_detection_model.joblib\n"
          ]
        }
      ]
    }
  ]
}